
import logging
import pprint
import lightgbm as lgb
import numpy as np
import shap
from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, auc
import streamlit as st
import pandas as pd 

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV, train_test_split

from scipy.stats import ks_2samp

from common_code import CommonCode

from controllers.data_builder import DatasetBuilder
from controllers.data_visualizer import DataVisualizer

from ui.dashboard_section import show_delinquency_ratio
from ui.dashboard_section import show_shap_analysis
from ui.dashboard_section import show_confusion_matrix
from ui.dashboard_section import show_performance_summary
from ui.dashboard_section import show_roc_curve


def run_scoring(builder: DatasetBuilder,visualizer : DataVisualizer, ê¸°ì¤€_ëŒ€ì¶œ, ë¹„êµ_ëŒ€ì¶œ, ì„ íƒë³€ìˆ˜, ì¡°íšŒì‹œì‘ë…„ì›”, ì¡°íšŒì¢…ë£Œë…„ì›”):

    logging.info("ì˜ˆì¸¡ëª¨ë¸ë§ ê°œìˆ˜")
    logging.info(len(builder.df_ì˜ˆì¸¡ëª¨ë¸ë§))
    logging.info(builder.df_ì˜ˆì¸¡ëª¨ë¸ë§.info())

    logging.info("ê¸°ì¤€ëŒ€ì¶œ : " + pprint.pformat(ê¸°ì¤€_ëŒ€ì¶œ))
    logging.info("ë¹„êµëŒ€ì¶œ : " + pprint.pformat(ë¹„êµ_ëŒ€ì¶œ))
    logging.info("ì„ íƒë³€ìˆ˜ : " + pprint.pformat(ì„ íƒë³€ìˆ˜))
    logging.info("ì„ íƒë³€ìˆ˜ : " + pprint.pformat(ì¡°íšŒì‹œì‘ë…„ì›”))
    logging.info("ì„ íƒë³€ìˆ˜ : " + pprint.pformat(ì¡°íšŒì¢…ë£Œë…„ì›”))

    #ê¸°ì¤€ì§‘ë‹¨
    df_base = builder.df_ì˜ˆì¸¡ëª¨ë¸ë§[
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_CD_1'] == str(ê¸°ì¤€_ëŒ€ì¶œ['ëŒ€ì¶œê³¼ëª©']['LN_CD_1']).lstrip('0')) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_CD_2'] == ê¸°ì¤€_ëŒ€ì¶œ['ëŒ€ì¶œê³¼ëª©']['LN_CD_2']) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_CD_3'] == ê¸°ì¤€_ëŒ€ì¶œ['ëŒ€ì¶œê³¼ëª©']['LN_CD_3']) &
        #(builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['ì§€ê¸‰ëŠ¥ë ¥_êµ¬ê°„'] == ê¸°ì¤€_ëŒ€ì¶œ['ì§€ê¸‰ëŠ¥ë ¥_êµ¬ê°„']) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['RATE'] <= int(ê¸°ì¤€_ëŒ€ì¶œ['ê¸ˆë¦¬']*1000)) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_AMT'] <= ê¸°ì¤€_ëŒ€ì¶œ['ëŒ€ì¶œê¸ˆì•¡(ì²œì›)'])
    ]

    # ì¡°íšŒë…„ì›” í•„í„°ë§ 
    df_base = df_base[
        (df_base['YM'] >= int(ì¡°íšŒì‹œì‘ë…„ì›”)) &
        (df_base['YM'] <= int(ì¡°íšŒì¢…ë£Œë…„ì›”))
    ]


    #ë¹„êµì§‘ë‹¨
    df_comp = builder.df_ì˜ˆì¸¡ëª¨ë¸ë§[
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_CD_1'] == str(ë¹„êµ_ëŒ€ì¶œ['ëŒ€ì¶œê³¼ëª©']['LN_CD_1']).lstrip('0')) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_CD_2'] == ë¹„êµ_ëŒ€ì¶œ['ëŒ€ì¶œê³¼ëª©']['LN_CD_2']) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_CD_3'] == ë¹„êµ_ëŒ€ì¶œ['ëŒ€ì¶œê³¼ëª©']['LN_CD_3']) &
        #(builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['ì§€ê¸‰ëŠ¥ë ¥_êµ¬ê°„'] == ë¹„êµ_ëŒ€ì¶œ['ì§€ê¸‰ëŠ¥ë ¥_êµ¬ê°„']) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['RATE'] <= int(ë¹„êµ_ëŒ€ì¶œ['ê¸ˆë¦¬']*1000)) &
        (builder.df_ì˜ˆì¸¡ëª¨ë¸ë§['LN_AMT'] <= int(ë¹„êµ_ëŒ€ì¶œ['ëŒ€ì¶œê¸ˆì•¡(ì²œì›)']))
    ]

    # ì¡°íšŒë…„ì›” í•„í„°ë§ 
    df_comp = df_comp[
        (df_comp['YM'] >= int(ì¡°íšŒì‹œì‘ë…„ì›”)) &
        (df_comp['YM'] <= int(ì¡°íšŒì¢…ë£Œë…„ì›”))
    ]

    logging.info("ê¸°ì¤€ëŒ€ì¶œ : "+pprint.pformat(df_base))
    logging.info("ë¹„êµëŒ€ì¶œ : "+pprint.pformat(df_comp))

    # ì°¨ì£¼ìˆ˜ ì œí•œ 
    if len(df_base) < 1000 or len(df_comp) < 1000:
        st.warning(f"""
        âš ï¸ ì§‘ë‹¨ì˜ row ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ì¡°ê±´ì„ ì¡°ì •í•˜ì„¸ìš”.

        - ê¸°ì¤€ëŒ€ì¶œ: {len(df_base):,}ê±´  
        - ë¹„êµëŒ€ì¶œ: {len(df_comp):,}ê±´
        """)
        st.stop()  # ì´í›„ ì‹¤í–‰ ì¤‘ë‹¨
    else : 
        with st.container():
            st.markdown("###  ë°ì´í„° ê±´ìˆ˜ ìš”ì•½")
            st.markdown(f"""
            - **ê¸°ì¤€ëŒ€ì¶œ:** {len(df_base):,}ê±´  
            - **ë¹„êµëŒ€ì¶œ:** {len(df_comp):,}ê±´
            """)

            st.markdown("### ê¸°ì¤€ëŒ€ì¶œ ì¡°íšŒì¡°ê±´")
            st.code(pprint.pformat(ê¸°ì¤€_ëŒ€ì¶œ), language="python")

            st.markdown("### ë¹„êµëŒ€ì¶œ ì¡°íšŒì¡°ê±´")
            st.code(pprint.pformat(ë¹„êµ_ëŒ€ì¶œ), language="python")

            st.markdown("### ì„ íƒë³€ìˆ˜")
            st.code(ì„ íƒë³€ìˆ˜, language="python")

    # ë¨¸ì‹ ëŸ¬ë‹ ì‹¤í–‰
    X_base_train, X_base_test, y_base_train, y_base_test = train_test_split(
        df_base[ì„ íƒë³€ìˆ˜], df_base['DLQ_YN'], test_size=0.2, random_state=42)

    X_comp_train, X_comp_test, y_comp_train, y_comp_test = train_test_split(
        df_comp[ì„ íƒë³€ìˆ˜], df_comp['DLQ_YN'], test_size=0.2, random_state=42)
    

    # 2. ì˜¤ë²„ìƒ˜í”Œë§ (SMOTE)
    smote = SMOTE(random_state=42)

    X_base_train_res, y_base_train_res = smote.fit_resample(X_base_train, y_base_train)
    X_comp_train_res, y_comp_train_res = smote.fit_resample(X_comp_train, y_comp_train)

    # 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (RandomizedSearchCV)
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.05, 0.1],
        'num_leaves': [15, 31],
        'max_depth': [3, 5],
        'min_child_samples': [10, 20],
        'subsample': [0.8],
        'colsample_bytree': [0.8]
    }

    clf_base = RandomizedSearchCV(
        estimator=lgb.LGBMClassifier(random_state=42),
        param_distributions=param_grid,
        n_iter=3,  # íƒìƒ‰ íšŸìˆ˜
        cv=2,
        scoring='roc_auc',
        verbose=1,
        random_state=42,
        n_jobs=-1
    )

    clf_comp = RandomizedSearchCV(
        estimator=lgb.LGBMClassifier(random_state=42),
        param_distributions=param_grid,
        n_iter=3,
        cv=2,
        scoring='roc_auc',
        verbose=1,
        random_state=42,
        n_jobs=-1
    )

    builder.clf_base_model = clf_base

    # 4. ëª¨ë¸ í•™ìŠµ
    clf_base.fit(X_base_train_res, y_base_train_res)
    clf_comp.fit(X_comp_train_res, y_comp_train_res)

    # ì˜ˆì¸¡ í™•ë¥ 
    y_base_proba = clf_base.predict_proba(X_base_test)[:, 1]
    y_comp_proba = clf_comp.predict_proba(X_comp_test)[:, 1]

    # ROC ê³„ì‚°
    fpr_base, tpr_base, _ = roc_curve(y_base_test, y_base_proba)
    roc_auc_base = auc(fpr_base, tpr_base)

    fpr_comp, tpr_comp, _ = roc_curve(y_comp_test, y_comp_proba)
    roc_auc_comp = auc(fpr_comp, tpr_comp)

    # streamlit ìš© df ìƒì„± 
    columns = ['ëŒ€ì¶œê±´ìˆ˜', 'ì¥ê¸°ê³ ì•¡ëŒ€ì¶œê±´ìˆ˜', 'ëŒ€ì¶œê¸ˆì•¡í•©', 'ì§€ê¸‰ëŠ¥ë ¥', 'ì§€ê¸‰ëŠ¥ë ¥_êµ¬ê°„', 'ë³´í—˜ê±´ìˆ˜', 'ë³´í—˜ì›”ë‚©ì…ì•¡', 'ì—°ì²´ê±´ìˆ˜', 'ì¥ê¸°ì—°ì²´ê±´ìˆ˜', 'ì—°ì²´ê¸ˆì•¡í•©', 'ì‹ ìš©ì¹´ë“œê°œìˆ˜', 'ì‹ ìš©ì¹´ë“œ_ì‚¬ìš©ë¥ _ì¦ê°€ëŸ‰', 'í˜„ê¸ˆì„œë¹„ìŠ¤_ì‚¬ìš©ë¥ _ì¦ê°€ëŸ‰','DLQ_YN']
    
    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ 
    builder.df_css_base = df_base[columns].copy()
    builder.df_css_comp = df_comp[columns].copy()

    builder.df_css_base.rename(columns={"DLQ_YN": "ì‹¤ì œ_ì—°ì²´ì—¬ë¶€"}, inplace=True)

    # X , Y 
    X_base = builder.df_css_base[ì„ íƒë³€ìˆ˜]
    Y_base = builder.df_css_base['ì‹¤ì œ_ì—°ì²´ì—¬ë¶€']

    builder.df_css_base['ì˜ˆì¸¡_ì—°ì²´í™•ë¥ '] = clf_base.predict_proba(X_base)[:, 1]

    # ìµœì  ì„ê³„ê°’ ì‚°ì¶œ 
    prec, recall, thresholds = precision_recall_curve(Y_base, builder.df_css_base['ì˜ˆì¸¡_ì—°ì²´í™•ë¥ '])
    f1 = 2 * (prec * recall) / (prec + recall + 1e-6)
    builder.best_threshold = thresholds[np.argmax(f1)]

    # threshold
    builder.df_css_base['ëŒ€ì¶œìŠ¹ì¸_ì„ê³„ê°’'] = builder.best_threshold

    # ì˜ˆì¸¡ê°’ ìƒì„± 
    builder.df_css_base['ì˜ˆì¸¡_ì—°ì²´ì—¬ë¶€'] = (builder.df_css_base["ì˜ˆì¸¡_ì—°ì²´í™•ë¥ "] >= builder.best_threshold).astype(int)
    builder.df_css_base['ëŒ€ìŠ¬_ìŠ¹ì¸ì—¬ë¶€'] = 1- builder.df_css_base['ì˜ˆì¸¡_ì—°ì²´ì—¬ë¶€']

    # ì‹œê°í™” 
    # íƒ­ ìƒì„±
    # css ì£¼ì…
    st.markdown("""
    <style>
    /* íƒ­ ì˜ì—­ ì „ì²´ ìŠ¤íƒ€ì¼ */
    div[data-baseweb="tab-list"] {
        font-size: 20px !important;    /* ê¸€ì í¬ê¸° */
        height: 60px;                  /* íƒ­ ë†’ì´ */
    }

    /* ê° íƒ­ ë²„íŠ¼ì˜ ìŠ¤íƒ€ì¼ */
    button[role="tab"] {
        padding: 12px 24px !important;  /* ë‚´ë¶€ ì—¬ë°± */
        font-size: 18px !important;     /* ê¸€ì í¬ê¸° */
        font-weight: bold !important;
        color: white !important;
        background-color: #1c1c1c !important;
        border-radius: 8px !important;
        margin-right: 10px !important;
    }

    /* ì„ íƒëœ íƒ­ ìŠ¤íƒ€ì¼ */
    button[aria-selected="true"] {
        background-color: #0e76a8 !important; /* ì„ íƒëœ íƒ­ ë°°ê²½ìƒ‰ */
        color: white !important;
    }
    </style>
    """, unsafe_allow_html=True)

    íƒ­1, íƒ­2, íƒ­3 = st.tabs(["ğŸ“ˆ ì—°ì²´ìœ¨ ì¶”ì´", "ğŸ” ë³€ìˆ˜ì¤‘ìš”ë„ ë¹„êµ", "ğŸ” ROC-Curve"])

    # ì²« ë²ˆì§¸ íƒ­: ì—°ì²´ìœ¨ ì¶”ì´
    with íƒ­1:
        st.subheader("ì—°ì²´ìœ¨ ì¶”ì´")
        show_delinquency_ratio(df_base, df_comp, ì¡°íšŒì‹œì‘ë…„ì›”, ì¡°íšŒì¢…ë£Œë…„ì›”)

    # ë‘ ë²ˆì§¸ íƒ­: ë³€ìˆ˜ì¤‘ìš”ë„
    with íƒ­2:
        st.subheader("ë³€ìˆ˜ì¤‘ìš”ë„ ë¹„êµ")

        explainer1 = shap.Explainer(clf_base.best_estimator_)
        explainer2 = shap.Explainer(clf_comp.best_estimator_)

        shap_values_base = explainer1.shap_values(X_base_test)
        shap_values_comp = explainer2.shap_values(X_comp_test)

        # explainer1 builder ì €ì¥ 
        builder.explainer = explainer1

        # builder ì— shap ê°’ ë¶€ì—¬ 
        shap_df = pd.DataFrame(shap_values_base, columns=builder.ì„ íƒë³€ìˆ˜)

        # ì¤‘ìš”ë„ ê³„ì‚° 
        shap_importance = shap_df[builder.ì„ íƒë³€ìˆ˜].abs().mean()
        builder.shap_weight_df = (shap_importance / shap_importance.mean()).round(2)

        # ì „ì²´ ì˜ˆì¸¡ í™•ë¥  (ì „ì²´ ëª¨ë¸ ê¸°ì¤€)
        auc_total = roc_auc_score(y_base_test, y_base_proba)
        ks_total = ks_2samp(y_base_proba[y_base_test == 1], y_base_proba[y_base_test == 0]).statistic

        # SHAPê°’ ê³„ì‚°
        explainer = shap.TreeExplainer(clf_base.best_estimator_)
        shap_values = explainer.shap_values(X_base_test)  

        # Gain ê°’
        feature_gain = clf_base.best_estimator_.booster_.feature_importance(importance_type='gain')
        gain_dict = dict(zip(X_base_test.columns, feature_gain))

        rows = []
        # ë³€ìˆ˜ë³„ ì„±ëŠ¥ ì •ë¦¬
        for i, col in enumerate(ì„ íƒë³€ìˆ˜):
            x = X_base_test[col]
            auc_val = roc_auc_score(y_base_test, x)
            ks = ks_2samp(x[y_base_test == 1], x[y_base_test == 0]).statistic
            shap_mean = np.abs(shap_values[:, i]).mean()
            gain = gain_dict.get(col, 0)

            rows.append({
                'ë³€ìˆ˜': col,
                'AUC (ë‹¨ë³€ìˆ˜)': round(auc_val, 4),
                'KS (ë‹¨ë³€ìˆ˜)': round(ks, 4),
                'SHAP': round(shap_mean, 4),
                'Gain': round(gain, 2)
            })
        
        
        # ì „ì²´ ëª¨ë¸ ê¸°ì¤€ ì„±ëŠ¥ ì¶”ê°€
        rows.append({
            'ë³€ìˆ˜': 'ì „ì²´ëª¨ë¸',
            'AUC (ë‹¨ë³€ìˆ˜)': round(auc_total, 4),
            'KS (ë‹¨ë³€ìˆ˜)': round(ks_total, 4),
            'SHAP': np.abs(shap_values).mean().round(4),
            'Gain': np.sum(feature_gain).round(2)
        })

        df_summary = pd.DataFrame(rows)
        builder.df_summary_wo = df_summary[df_summary['ë³€ìˆ˜'] != 'ì „ì²´ëª¨ë¸']

        show_shap_analysis(clf_base.best_estimator_, clf_comp.best_estimator_, X_base_test, X_comp_test)

        st.subheader("ê¸°ì¤€ëŒ€ì¶œ ë³€ìˆ˜ì¤‘ìš”ë„(Matrix)")
        show_performance_summary(df_summary,"ê¸°ì¤€ëŒ€ì¶œ")

        st.subheader("ë¹„êµëŒ€ì¶œ ë³€ìˆ˜ì¤‘ìš”ë„(Matrix)")
        show_performance_summary(df_summary,"ë¹„êµëŒ€ì¶œ")

    with íƒ­3:
        st.subheader("ROC Curve")
        show_roc_curve(fpr_base,tpr_base,fpr_comp,tpr_comp,roc_auc_base,roc_auc_comp)

        col1, col2 = st.columns(2)

        with col1: 
            st.subheader("Confusion Matrix(ê¸°ì¤€ëŒ€ì¶œ)")
            show_confusion_matrix(clf_base,X_base_test,y_base_test)
        with col2:
            st.subheader("Confusion Matrix(ë¹„êµëŒ€ì¶œ)")
            show_confusion_matrix(clf_comp,X_comp_test,y_comp_test)